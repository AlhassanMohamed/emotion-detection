{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def __init__(self,sizes):\n",
      "\t\t\"\"\"\n",
      "\t\tSizes is a list of the number of neurons in each layer of the neural \n",
      "\t\tnetwork. For example, if sizes = [3,2,3], that would indicate that the\n",
      "\t\tneural network has three layers, the first with 3 neurons, the second \n",
      "\t\twith 2 neurons, and the third with 3 neurons. \n",
      "\n",
      "\t\tThe function initializes the parameters and framework for the network.\n",
      "\t\t\"\"\"\n",
      "\n",
      "\t\tself.numlayers = len(self.sizes)\n",
      "\t\tself.sizes = sizes\n",
      "\t\tself.numlayers = len(sizes)\n",
      "\t\tself.weights = [np.random.randn((self.sizes[0],self.sizes[1])), \n",
      "\t\t  np.random.randn((self.sizes[1],self.sizes[2]))]\n",
      "\t\tself.biasess = [np.random.randn(self.sizes[1]), np.random.randn(self.sizes[2])]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import random\n",
      "import numpy as np\n",
      "\n",
      "class Net(object):\n",
      "\tdef __init__(self,sizes):\n",
      "\t\t\"\"\"\n",
      "\t\tSizes is a list of the number of neurons in each layer of the neural \n",
      "\t\tnetwork. For example, if sizes = [3,2,3], that would indicate that the\n",
      "\t\tneural network has three layers, the first with 3 neurons, the second \n",
      "\t\twith 2 neurons, and the third with 3 neurons. \n",
      "\n",
      "\t\tThe function initializes the parameters and framework for the network.\n",
      "\t\t\"\"\"\n",
      "\n",
      "\t\tself.numlayers = len(self.sizes)\n",
      "\t\tself.sizes = sizes\n",
      "\t\tself.numlayers = len(sizes)\n",
      "\t\tself.weights = [np.random.randn((self.sizes[0],self.sizes[1])), \n",
      "\t\t  np.random.randn((self.sizes[1],self.sizes[2]))]\n",
      "\t\tself.biasess = [np.random.randn(self.sizes[1]), np.random.randn(self.sizes[2])]\n",
      "\n",
      "\n",
      "\tdef SGD(self, train_data, epochs, mini_batch_size, eta):\n",
      "\t\t\"\"\"\n",
      "\t\tTrains the network using mini-batch stochastic gradient descent. \n",
      "\t\tParameters:\n",
      "\t\t\ttrain_data: A vector of the data used to train the model. Each element\n",
      "\t\t\t\tin the vector is of the form (x,y) where x is the training data and\n",
      "\t\t\t\ty is the correct output. \n",
      "\n",
      "\t\t\tepochs: The number of epochs in the training.\n",
      "\n",
      "\t\t\tmini_batch_size: The number of mini-batches. If we have some set of \n",
      "\t\t\t\ttraining data inputs, we partition the set of training data inputs\n",
      "\t\t\t\tinto sets of mini_batch_size. The gradient of the cost function\n",
      "\t\t\t\tis calculated for the mini-batch, from which we calculate the \n",
      "\t\t\t\tgradient for the entire training set and the new weights and biases\n",
      "\t\t\t\tare calculated.\n",
      "\n",
      "\t\t\teta: The constant that determines how much the weights and biases\n",
      "\t\t\t\tchange in the direction of the calculated gradient. \n",
      "\t\t\"\"\"\n",
      "\t\tfor i in range(epochs):\n",
      "\t\t\trandom.shuffle(train_data) # shuffle the data to randomly devide it into mini batches later\n",
      "\t\t\tcur = 0\n",
      "\t\t\twhile cur < len(train_data):\n",
      "\t\t\t\tmini_batch = train_data[cur : cur + mini_batch_size]\n",
      "\t\t\t\tcur += mini_batch_size\n",
      "\t\t\t\tupdate_weights(mini_batch, eta)\n",
      "\n",
      "\tdef update_weights (self, mini_batch, eta):\n",
      "\t\t\"\"\"\n",
      "\t\tUpdates the weights and biases by applying gradient descent to the \n",
      "\t\tmini-batch passed into the function. \n",
      "\t\t\"\"\"\n",
      "\t\tdelta_weights = [np.zeros((self.sizes[0],self.sizes[1])), np.zeros((self.sizes[1],self.sizes[2]))]\n",
      "\t\tdelta_biases = [np.zeros(self.sizes[1]), np.zeros(self.sizes[2])]\n",
      "\t\t\n",
      "\t\tfor x, y in mini_batch:\n",
      "\t\t\tdelta_weights_temp, delta_biases_temp = backprop(x, y)\n",
      "\t\t\tfor i in range(self.numlayers):\n",
      "\t\t\t\tdelta_weights[i] += delta_weights_temp[i]\n",
      "\t\t\t\tdelta_biases[i] += delta_biases_temp[i]\n",
      "\t\t\n",
      "\t\t# adjust the weights\n",
      "\t\tfor i in range(self.numlayers):\n",
      "\t\t\tself.weights[i] += delta_weights[i]\n",
      "\t\t\tself.biases[i] += delta_biases[i]\n",
      "\n",
      "\tdef backprop(self,x,y):\n",
      "\t\t\"\"\"\n",
      "\t\tUses backpropagation to calculate the gradient of the cost function\n",
      "\n",
      "\t\tBackpropagation consists of the following five steps:\n",
      "\t\t\t1) Set the input of the network to the training data. \n",
      "\t\t\t\n",
      "\t\t\t2) Use the feedforward function to compute the activations\n",
      "\t\t\tat each layer of the network given the input.\n",
      "\n",
      "\t\t\t3) Compute each error vector for the last layer before the output\n",
      "\t\t\tlayer by using the rate of change of the cost as a function of the \n",
      "\t\t\tcorresponding output and the derivative of the activation function\n",
      "\t\t\tevaluated at the last layer neuron. \n",
      "\n",
      "\t\t\t4) Using each error value in the last layer, calculate the error\n",
      "\t\t\tvalues in each of the previous layers. \n",
      "\n",
      "\t\t\t5) Calculate the gradient of the cost function for each weight and\n",
      "\t\t\tfor the biases using the error values, using the formula in the spec.\n",
      "\t\t\t\n",
      "\t\tSet activation to a list containing x  # x is the input activation\n",
      "\t\tSet zlist to empty list  # zlist is the list of z activations\n",
      "\t\t\"\"\"\n",
      "\t\tsig_prime_vec = np.vectorize(sig_prime)\n",
      "\t\tsig_vec = np.vectorize(sig)\n",
      "\n",
      "\t\tactivation = []\n",
      "\t\tactivation.append(x)\n",
      "\t\tzlist = []\n",
      "\t\tzlist.append(x)\n",
      "\n",
      "\t\tfor i in range(numlayers - 1):\n",
      "\t\t\ta = np.dot(self.weights[i],x) + self.biases[i]\n",
      "\t\t\tz = sig_vec(a)\n",
      "\t\t\tactivation.append(a)\n",
      "\t\t\tzlist.append(z)\n",
      "\n",
      "\t\toutput = z\n",
      "\n",
      "\t\tgradcost = a - y  # Gradient of the cost function with respect to the activation outputs\n",
      "\n",
      "\t\tdelt_l = gradcost*sig_prime_vec(output) # Error delta for last layer\n",
      "\t\tdelts = []\n",
      "\t\tdelts.append(delt_l)\n",
      "\n",
      "\t\tfor i in range(2, numlayers+1): # Calculate errors for previous layers\n",
      "\t\t\tdelt = (np.dot(np.tranpose(weights[numlayers-i]),delts[i])*\n",
      "\t\t\t\t\tsig_prime_vec(zlist[numlayers-i]))\n",
      "\t\t\tdelts.append(delt)\n",
      "\n",
      "\t\tdelts = delts.reverse\n",
      "\t\tgrad_biases = delts.pop(0) # Calculate the partial derivatives of the cost function wrt to biases\n",
      "\n",
      "\t\tgrad_weights = []\n",
      "\t\tfor i in range(1, numlayers): # Calculate the gradient with respect to the weights\n",
      "\t\t\tgweight = np.dot(delts[i],activation[i-1])\n",
      "\t\t\tgrad_weights.append(gweight)\n",
      "\n",
      "\t\treturn(grad_weights,grad_biases)\n",
      "\n",
      "\tdef evaluate(self,test_data):\n",
      "\t\traise TODO\n",
      "\n",
      "\n",
      "def sig(x):\n",
      "\treturn 1.0/(1.0 + np.exp(-x))\n",
      "\n",
      "def sig_prime(x):\n",
      "\treturn sig(x) * (1-sig(x))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "self.weights = [np.random.randn((self.sizes[0],self.sizes[1])), \n",
      "\t\t  np.random.randn((self.sizes[1],self.sizes[2]))]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "NameError",
       "evalue": "name 'self' is not defined",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-3-dd06fdc9734f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m self.weights = [np.random.randn((self.sizes[0],self.sizes[1])), \n\u001b[0m\u001b[1;32m      2\u001b[0m \t\t  np.random.randn((self.sizes[1],self.sizes[2]))]\n",
        "\u001b[0;31mNameError\u001b[0m: name 'self' is not defined"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "np.random.randn(2,3)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 5,
       "text": [
        "array([[-0.63734478, -0.62143733,  1.16965261],\n",
        "       [-2.1775594 , -1.00374059, -0.3731047 ]])"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}