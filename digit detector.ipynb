{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# -*- coding: utf-8 -*-\n",
      "# <nbformat>3.0</nbformat>\n",
      "\n",
      "# <codecell>\n",
      "\n",
      "# import net\n",
      "from array import array\n",
      "import gzip\n",
      "import struct\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "# <codecell>\n",
      "\n",
      "# open images\n",
      "f = gzip.open('./train-images-idx3-ubyte.gz', 'rb') # test set\n",
      "_, n_images, rows, cols = struct.unpack('>IIII', f.read(16)) # first 16 bytes are metadata\n",
      "pixels = np.array(array('B', f.read())) / 255.0 # pixel data are unsigned bytes; normalize to [0,1] scale\n",
      "f.close()\n",
      "images = pixels.reshape((n_images, rows * cols)) # each image is a vector of 28*28 values in [0,1]\n",
      "\n",
      "# <codecell>\n",
      "\n",
      "f = gzip.open('./train-labels-idx1-ubyte.gz', 'rb')\n",
      "_, n_images = struct.unpack('>II', f.read(8)) # first 16 bytes are metadata\n",
      "digits = np.array(array('B', f.read())) \n",
      "f.close()\n",
      "\n",
      "# <codecell>\n",
      "\n",
      "labels = np.zeros((n_images, 10))\n",
      "for i in range(n_images):\n",
      "    labels[i][digits[i]] = 1"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# <codecell>\n",
      "temp_images = images[0:1]\n",
      "temp_labels = labels[0:1]\n",
      "train_data = zip(temp_images, temp_labels)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 273
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "digits[0:10]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 252,
       "text": [
        "array([5, 0, 4, 1, 9, 2, 1, 3, 1, 4])"
       ]
      }
     ],
     "prompt_number": 252
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# <codecell>\n",
      "detector = Net([rows * cols, 400, 10])\n",
      "detector.SGD(train_data, 1, 1, 0.6)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "epoch 0\n",
        "5\n"
       ]
      }
     ],
     "prompt_number": 274
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "img = images[1032]\n",
      "# mysig = np.vectorize(sig)\n",
      "# mysig(np.dot(mysig(np.dot(img, detector.weights[0]) + detector.biases[0]), detector.weights[1]) + detector.biases[1])\n",
      "np.dot((np.dot(img, detector.weights[0]) + detector.biases[0]), detector.weights[1]) + detector.biases[1]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 242,
       "text": [
        "array([-0.6 , -0.55, -0.65, -0.6 , -0.6 , -0.55, -0.6 , -0.6 , -0.65, -0.6 ])"
       ]
      }
     ],
     "prompt_number": 242
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "detector.biases"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 259,
       "text": [
        "[array([ -5.83889512e-27,  -5.83889512e-27,  -5.83889512e-27,\n",
        "        -5.83889512e-27,  -5.83889512e-27,  -5.83889512e-27,\n",
        "        -5.83889512e-27,  -5.83889512e-27,  -5.83889512e-27,\n",
        "        -5.83889512e-27,  -5.83889512e-27,  -5.83889512e-27,\n",
        "        -5.83889512e-27,  -5.83889512e-27,  -5.83889512e-27,\n",
        "        -5.83889512e-27,  -5.83889512e-27,  -5.83889512e-27,\n",
        "        -5.83889512e-27,  -5.83889512e-27,  -5.83889512e-27,\n",
        "        -5.83889512e-27,  -5.83889512e-27,  -5.83889512e-27,\n",
        "        -5.83889512e-27,  -5.83889512e-27,  -5.83889512e-27,\n",
        "        -5.83889512e-27,  -5.83889512e-27,  -5.83889512e-27,\n",
        "        -5.83889512e-27,  -5.83889512e-27,  -5.83889512e-27,\n",
        "        -5.83889512e-27,  -5.83889512e-27,  -5.83889512e-27,\n",
        "        -5.83889512e-27,  -5.83889512e-27,  -5.83889512e-27,\n",
        "        -5.83889512e-27,  -5.83889512e-27,  -5.83889512e-27,\n",
        "        -5.83889512e-27,  -5.83889512e-27,  -5.83889512e-27,\n",
        "        -5.83889512e-27,  -5.83889512e-27,  -5.83889512e-27,\n",
        "        -5.83889512e-27,  -5.83889512e-27,  -5.83889512e-27,\n",
        "        -5.83889512e-27,  -5.83889512e-27,  -5.83889512e-27,\n",
        "        -5.83889512e-27,  -5.83889512e-27,  -5.83889512e-27,\n",
        "        -5.83889512e-27,  -5.83889512e-27,  -5.83889512e-27,\n",
        "        -5.83889512e-27,  -5.83889512e-27,  -5.83889512e-27,\n",
        "        -5.83889512e-27,  -5.83889512e-27,  -5.83889512e-27,\n",
        "        -5.83889512e-27,  -5.83889512e-27,  -5.83889512e-27,\n",
        "        -5.83889512e-27,  -5.83889512e-27,  -5.83889512e-27,\n",
        "        -5.83889512e-27,  -5.83889512e-27,  -5.83889512e-27,\n",
        "        -5.83889512e-27,  -5.83889512e-27,  -5.83889512e-27,\n",
        "        -5.83889512e-27,  -5.83889512e-27,  -5.83889512e-27,\n",
        "        -5.83889512e-27,  -5.83889512e-27,  -5.83889512e-27,\n",
        "        -5.83889512e-27,  -5.83889512e-27,  -5.83889512e-27,\n",
        "        -5.83889512e-27,  -5.83889512e-27,  -5.83889512e-27,\n",
        "        -5.83889512e-27,  -5.83889512e-27,  -5.83889512e-27,\n",
        "        -5.83889512e-27,  -5.83889512e-27,  -5.83889512e-27,\n",
        "        -5.83889512e-27,  -5.83889512e-27,  -5.83889512e-27,\n",
        "        -5.83889512e-27,  -5.83889512e-27,  -5.83889512e-27,\n",
        "        -5.83889512e-27,  -5.83889512e-27,  -5.83889512e-27,\n",
        "        -5.83889512e-27,  -5.83889512e-27,  -5.83889512e-27,\n",
        "        -5.83889512e-27,  -5.83889512e-27,  -5.83889512e-27,\n",
        "        -5.83889512e-27,  -5.83889512e-27,  -5.83889512e-27,\n",
        "        -5.83889512e-27,  -5.83889512e-27,  -5.83889512e-27,\n",
        "        -5.83889512e-27,  -5.83889512e-27,  -5.83889512e-27,\n",
        "        -5.83889512e-27,  -5.83889512e-27,  -5.83889512e-27,\n",
        "        -5.83889512e-27,  -5.83889512e-27,  -5.83889512e-27,\n",
        "        -5.83889512e-27,  -5.83889512e-27,  -5.83889512e-27,\n",
        "        -5.83889512e-27,  -5.83889512e-27,  -5.83889512e-27,\n",
        "        -5.83889512e-27,  -5.83889512e-27,  -5.83889512e-27,\n",
        "        -5.83889512e-27,  -5.83889512e-27,  -5.83889512e-27,\n",
        "        -5.83889512e-27,  -5.83889512e-27,  -5.83889512e-27,\n",
        "        -5.83889512e-27,  -5.83889512e-27,  -5.83889512e-27,\n",
        "        -5.83889512e-27,  -5.83889512e-27,  -5.83889512e-27,\n",
        "        -5.83889512e-27,  -5.83889512e-27,  -5.83889512e-27,\n",
        "        -5.83889512e-27,  -5.83889512e-27,  -5.83889512e-27,\n",
        "        -5.83889512e-27,  -5.83889512e-27,  -5.83889512e-27,\n",
        "        -5.83889512e-27,  -5.83889512e-27,  -5.83889512e-27,\n",
        "        -5.83889512e-27,  -5.83889512e-27,  -5.83889512e-27,\n",
        "        -5.83889512e-27,  -5.83889512e-27,  -5.83889512e-27,\n",
        "        -5.83889512e-27,  -5.83889512e-27,  -5.83889512e-27,\n",
        "        -5.83889512e-27,  -5.83889512e-27,  -5.83889512e-27,\n",
        "        -5.83889512e-27,  -5.83889512e-27,  -5.83889512e-27,\n",
        "        -5.83889512e-27,  -5.83889512e-27,  -5.83889512e-27,\n",
        "        -5.83889512e-27,  -5.83889512e-27,  -5.83889512e-27,\n",
        "        -5.83889512e-27,  -5.83889512e-27,  -5.83889512e-27,\n",
        "        -5.83889512e-27,  -5.83889512e-27,  -5.83889512e-27,\n",
        "        -5.83889512e-27,  -5.83889512e-27,  -5.83889512e-27,\n",
        "        -5.83889512e-27,  -5.83889512e-27,  -5.83889512e-27,\n",
        "        -5.83889512e-27,  -5.83889512e-27,  -5.83889512e-27,\n",
        "        -5.83889512e-27,  -5.83889512e-27,  -5.83889512e-27,\n",
        "        -5.83889512e-27,  -5.83889512e-27,  -5.83889512e-27,\n",
        "        -5.83889512e-27,  -5.83889512e-27,  -5.83889512e-27,\n",
        "        -5.83889512e-27,  -5.83889512e-27,  -5.83889512e-27,\n",
        "        -5.83889512e-27,  -5.83889512e-27,  -5.83889512e-27,\n",
        "        -5.83889512e-27,  -5.83889512e-27,  -5.83889512e-27,\n",
        "        -5.83889512e-27,  -5.83889512e-27,  -5.83889512e-27,\n",
        "        -5.83889512e-27,  -5.83889512e-27,  -5.83889512e-27,\n",
        "        -5.83889512e-27,  -5.83889512e-27,  -5.83889512e-27,\n",
        "        -5.83889512e-27,  -5.83889512e-27,  -5.83889512e-27,\n",
        "        -5.83889512e-27,  -5.83889512e-27,  -5.83889512e-27,\n",
        "        -5.83889512e-27,  -5.83889512e-27,  -5.83889512e-27,\n",
        "        -5.83889512e-27,  -5.83889512e-27,  -5.83889512e-27,\n",
        "        -5.83889512e-27,  -5.83889512e-27,  -5.83889512e-27,\n",
        "        -5.83889512e-27,  -5.83889512e-27,  -5.83889512e-27,\n",
        "        -5.83889512e-27,  -5.83889512e-27,  -5.83889512e-27,\n",
        "        -5.83889512e-27,  -5.83889512e-27,  -5.83889512e-27,\n",
        "        -5.83889512e-27,  -5.83889512e-27,  -5.83889512e-27,\n",
        "        -5.83889512e-27,  -5.83889512e-27,  -5.83889512e-27,\n",
        "        -5.83889512e-27,  -5.83889512e-27,  -5.83889512e-27,\n",
        "        -5.83889512e-27,  -5.83889512e-27,  -5.83889512e-27,\n",
        "        -5.83889512e-27,  -5.83889512e-27,  -5.83889512e-27,\n",
        "        -5.83889512e-27,  -5.83889512e-27,  -5.83889512e-27,\n",
        "        -5.83889512e-27,  -5.83889512e-27,  -5.83889512e-27,\n",
        "        -5.83889512e-27,  -5.83889512e-27,  -5.83889512e-27,\n",
        "        -5.83889512e-27,  -5.83889512e-27,  -5.83889512e-27,\n",
        "        -5.83889512e-27,  -5.83889512e-27,  -5.83889512e-27,\n",
        "        -5.83889512e-27,  -5.83889512e-27,  -5.83889512e-27,\n",
        "        -5.83889512e-27,  -5.83889512e-27,  -5.83889512e-27,\n",
        "        -5.83889512e-27,  -5.83889512e-27,  -5.83889512e-27,\n",
        "        -5.83889512e-27,  -5.83889512e-27,  -5.83889512e-27,\n",
        "        -5.83889512e-27,  -5.83889512e-27,  -5.83889512e-27,\n",
        "        -5.83889512e-27,  -5.83889512e-27,  -5.83889512e-27,\n",
        "        -5.83889512e-27,  -5.83889512e-27,  -5.83889512e-27,\n",
        "        -5.83889512e-27,  -5.83889512e-27,  -5.83889512e-27,\n",
        "        -5.83889512e-27,  -5.83889512e-27,  -5.83889512e-27,\n",
        "        -5.83889512e-27,  -5.83889512e-27,  -5.83889512e-27,\n",
        "        -5.83889512e-27,  -5.83889512e-27,  -5.83889512e-27,\n",
        "        -5.83889512e-27,  -5.83889512e-27,  -5.83889512e-27,\n",
        "        -5.83889512e-27,  -5.83889512e-27,  -5.83889512e-27,\n",
        "        -5.83889512e-27,  -5.83889512e-27,  -5.83889512e-27,\n",
        "        -5.83889512e-27,  -5.83889512e-27,  -5.83889512e-27,\n",
        "        -5.83889512e-27,  -5.83889512e-27,  -5.83889512e-27,\n",
        "        -5.83889512e-27,  -5.83889512e-27,  -5.83889512e-27,\n",
        "        -5.83889512e-27,  -5.83889512e-27,  -5.83889512e-27,\n",
        "        -5.83889512e-27,  -5.83889512e-27,  -5.83889512e-27,\n",
        "        -5.83889512e-27,  -5.83889512e-27,  -5.83889512e-27,\n",
        "        -5.83889512e-27,  -5.83889512e-27,  -5.83889512e-27,\n",
        "        -5.83889512e-27,  -5.83889512e-27,  -5.83889512e-27,\n",
        "        -5.83889512e-27,  -5.83889512e-27,  -5.83889512e-27,\n",
        "        -5.83889512e-27,  -5.83889512e-27,  -5.83889512e-27,\n",
        "        -5.83889512e-27,  -5.83889512e-27,  -5.83889512e-27,\n",
        "        -5.83889512e-27,  -5.83889512e-27,  -5.83889512e-27,\n",
        "        -5.83889512e-27,  -5.83889512e-27,  -5.83889512e-27,\n",
        "        -5.83889512e-27,  -5.83889512e-27,  -5.83889512e-27,\n",
        "        -5.83889512e-27,  -5.83889512e-27,  -5.83889512e-27,\n",
        "        -5.83889512e-27,  -5.83889512e-27,  -5.83889512e-27,\n",
        "        -5.83889512e-27,  -5.83889512e-27,  -5.83889512e-27,\n",
        "        -5.83889512e-27,  -5.83889512e-27,  -5.83889512e-27,\n",
        "        -5.83889512e-27,  -5.83889512e-27,  -5.83889512e-27,\n",
        "        -5.83889512e-27,  -5.83889512e-27,  -5.83889512e-27,\n",
        "        -5.83889512e-27,  -5.83889512e-27,  -5.83889512e-27,\n",
        "        -5.83889512e-27,  -5.83889512e-27,  -5.83889512e-27,\n",
        "        -5.83889512e-27,  -5.83889512e-27,  -5.83889512e-27,\n",
        "        -5.83889512e-27,  -5.83889512e-27,  -5.83889512e-27,\n",
        "        -5.83889512e-27,  -5.83889512e-27,  -5.83889512e-27,\n",
        "        -5.83889512e-27,  -5.83889512e-27,  -5.83889512e-27,\n",
        "        -5.83889512e-27,  -5.83889512e-27,  -5.83889512e-27,\n",
        "        -5.83889512e-27]),\n",
        " array([-1.2, -0.6, -1.2, -1.2, -0.9, -1.2, -1.5, -1.5, -1.5, -1.2])]"
       ]
      }
     ],
     "prompt_number": 259
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# test the neural network\n",
      "%pylab inline\n",
      "tests = images[1000:1064]\n",
      "plt.figure(figsize=(10, 10))\n",
      "for k in xrange(len(tests)):\n",
      "    plt.subplot2grid((8, 8), (k / 8, k % 8))\n",
      "    plt.axis('off')\n",
      "    plt.imshow(1.0 - tests[k].reshape((rows, cols)), cmap=plt.cm.gray)\n",
      "    plt.title(detector.evaluate(tests[k]), fontsize=24)\n",
      "    print detector.evaluate(tests[k])\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 261
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import random\n",
      "import numpy as np\n",
      "\n",
      "class Net(object):\n",
      "    def __init__(self,sizes):\n",
      "        \"\"\"\n",
      "        Sizes is a list of the number of neurons in each layer of the neural \n",
      "        network. For example, if sizes = [3,2,3], that would indicate that the\n",
      "        neural network has three layers, the first with 3 neurons, the second \n",
      "        with 2 neurons, and the third with 3 neurons. \n",
      "\n",
      "        The function initializes the parameters and framework for the network.\n",
      "        \"\"\"\n",
      "\n",
      "        self.sizes = sizes\n",
      "        self.numlayers = len(sizes)\n",
      "        # self.weights = [np.zeros((sizes[i],sizes[i+1])) for i in range(self.numlayers - 1)]\n",
      "        # self.biases = [np.zeros(sizes[i + 1]) for i in range(self.numlayers - 1)] \n",
      "        self.weights = [np.random.randn(sizes[i],sizes[i+1]) for i in range(self.numlayers - 1)]\n",
      "        self.biases = [np.random.randn(sizes[i + 1]) for i in range(self.numlayers - 1)] \n",
      "        self.delts = []\n",
      "        self.activation = []\n",
      "        self.grad_weights = []\n",
      "\n",
      "    def SGD(self, train_data, epochs, mini_batch_size, eta):\n",
      "        \"\"\"\n",
      "        Trains the network using mini-batch stochastic gradient descent. \n",
      "        Parameters:\n",
      "            train_data: A vector of the data used to train the model. Each element\n",
      "                in the vector is of the form (x,y) where x is the training data and\n",
      "                y is the correct output. \n",
      "\n",
      "            epochs: The number of epochs in the training.\n",
      "\n",
      "            mini_batch_size: The number of mini-batches. If we have some set of \n",
      "                training data inputs, we partition the set of training data inputs\n",
      "                into sets of mini_batch_size. The gradient of the cost function\n",
      "                is calculated for the mini-batch, from which we calculate the \n",
      "                gradient for the entire training set and the new weights and biases\n",
      "                are calculated.\n",
      "\n",
      "            eta: The constant that determines how much the weights and biases\n",
      "                change in the direction of the calculated gradient. \n",
      "        \"\"\"\n",
      "        for i in range(epochs):\n",
      "            print \"epoch\", i\n",
      "            random.shuffle(train_data) # shuffle the data to randomly devide it into mini batches later\n",
      "            cur = 0\n",
      "            \n",
      "            # j = 0\n",
      "            while cur < len(train_data):\n",
      "                # print j,\n",
      "                # j += 1\n",
      "                mini_batch = train_data[cur : cur + mini_batch_size]\n",
      "                cur += mini_batch_size\n",
      "                self.update_weights(mini_batch, eta)\n",
      "\n",
      "    def update_weights (self, mini_batch, eta):\n",
      "        \"\"\"\n",
      "        Updates the weights and biases by applying gradient descent to the \n",
      "        mini-batch passed into the function. \n",
      "        \"\"\"\n",
      "        delta_weights = [np.zeros((self.sizes[i],self.sizes[i+1])) for i in range(self.numlayers - 1)]\n",
      "        delta_biases = [np.zeros(self.sizes[i + 1]) for i in range(self.numlayers - 1)] \n",
      "        \n",
      "        for x, y in mini_batch:\n",
      "            delta_weights_temp, delta_biases_temp = self.backprop(x, y)\n",
      "            # print \"backprop little delta biases\", delta_biases_temp[1][0]\n",
      "            for i in range(self.numlayers-1):\n",
      "                delta_weights[i] += delta_weights_temp[i]\n",
      "                delta_biases[i] += delta_biases_temp[i]\n",
      "        \n",
      "        # print \"summed delta biases\", delta_biases[0][300], \n",
      "        \n",
      "        # adjust the weights\n",
      "        for i in range(self.numlayers-1):\n",
      "            self.weights[i] += - eta * delta_weights[i]\n",
      "            # if i == 1:\n",
      "                # print \"before: biases\", self.biases[1][0]\n",
      "                # print \"change: \", - eta* delta_biases[1]\n",
      "            self.biases[i] += - eta * delta_biases[i]\n",
      "        \n",
      "        for x, y in mini_batch:\n",
      "            print self.evaluate(x),\n",
      "        # print \"after: biases\", self.biases[0][300]\n",
      "\n",
      "    def backprop(self,x,y):\n",
      "        \"\"\"\n",
      "        Uses backpropagation to calculate the gradient of the cost function\n",
      "\n",
      "        Backpropagation consists of the following five steps:\n",
      "            1) Set the input of the network to the training data. \n",
      "            \n",
      "            2) Use the feedforward function to compute the activations\n",
      "            at each layer of the network given the input.\n",
      "\n",
      "            3) Compute each error vector for the last layer before the output\n",
      "            layer by using the rate of change of the cost as a function of the \n",
      "            corresponding output and the derivative of the activation function\n",
      "            evaluated at the last layer neuron. \n",
      "\n",
      "            4) Using each error value in the last layer, calculate the error\n",
      "            values in each of the previous layers. \n",
      "\n",
      "            5) Calculate the gradient of the cost function for each weight and\n",
      "            for the biases using the error values, using the formula in the spec.\n",
      "            \n",
      "        Set activation to a list containing x  # x is the input activation\n",
      "        Set zlist to empty list  # zlist is the list of z activations\n",
      "        \"\"\"\n",
      "        numlayers = self.numlayers\n",
      "        sizes = self.sizes\n",
      "        \n",
      "        sig_prime_vec = np.vectorize(sig_prime)\n",
      "        sig_vec = np.vectorize(sig)\n",
      "        \n",
      "        zlist = [np.zeros(sizes[i]) for i in range(self.numlayers)] # weighted input before applying sigmoid function\n",
      "        zlist[0] = x # weighted input in the first layer is just the input\n",
      "        activation = [np.zeros(sizes[i]) for i in range(self.numlayers)] # output after applying sigmoid function\n",
      "        activation[0] = x\n",
      "        \n",
      "        # feedforward: compute the output in each layer\n",
      "        for i in range(1, numlayers):\n",
      "            # print \"weights\", self.weights[i - 1].shape, \"activation\", activation[i - 1].shape\n",
      "            zlist[i] = np.dot(activation[i - 1], self.weights[i - 1]) + self.biases[i - 1]\n",
      "            activation[i] = sig_vec(zlist[i])\n",
      "            \n",
      "        # print y\n",
      "        gradcost = activation[numlayers - 1] - y  # Gradient of the cost function with respect to the activation outputs\n",
      "        # print \"output norm\", np.linalg.norm(activation[numlayers - 1]),\"gradcost norm\", np.linalg.norm(gradcost), \"y\", np.linalg.norm(y) \n",
      "        \n",
      "        delts = [0 for i in range(numlayers)]\n",
      "        delts[self.numlayers - 1] = gradcost * activation[numlayers - 1] # last layer\n",
      "        \n",
      "        # Backward propagate: calculate errors for previous layers\n",
      "        for i in range(numlayers - 2, 0, -1): \n",
      "            # print self.weights[i].shape, delts[i + 1].shape\n",
      "            delts[i] = (np.dot(delts[i + 1], np.transpose(self.weights[i])) * sig_prime_vec(zlist[i]))\n",
      "        \n",
      "        # calculate the gradients\n",
      "        grad_weights = [np.zeros((sizes[i],sizes[i+1])) for i in range(self.numlayers - 1)]\n",
      "        for i in range(1, numlayers): # Calculate the partial derivatives of the cost function wrt the weights\n",
      "            # print i, \"delts[i]\", delts[i]\n",
      "            # print \"activation[i - 1]\", activation[i - 1]\n",
      "            grad_weights[i - 1] = np.dot(np.transpose(activation[i - 1].reshape((1,-1))), delts[i].reshape((1,-1))) # check later\n",
      "            if i == 1:\n",
      "                self.delts = delts[i]\n",
      "                self.activation =activation[i - 1]  \n",
      "                self.grad_weights = grad_weights[i - 1]\n",
      "            # print \"grad_weights\", grad_weights[i - 1]\n",
      "        grad_biases = delts[1:] # Calculate the partial derivatives of the cost function wrt biases\n",
      "\n",
      "        return(grad_weights,grad_biases)\n",
      "\n",
      "    def evaluate(self,test_data):\n",
      "        sig_prime_vec = np.vectorize(sig_prime)\n",
      "        sig_vec = np.vectorize(sig)\n",
      "\n",
      "        activation = test_data\n",
      "        \n",
      "        for i in range(1, self.numlayers):\n",
      "            z = np.dot(activation, self.weights[i - 1]) + self.biases[i - 1]\n",
      "            activation = sig_vec(z)\n",
      "\n",
      "        # return activation\n",
      "        return max(enumerate(activation),key=lambda x: x[1])[0] # return the index of the maximum element in output list\n",
      "\n",
      "\n",
      "def sig(x):\n",
      "    return 1.0 / (1.0 + np.exp(-x))\n",
      "\n",
      "def sig_prime(x):\n",
      "    return sig(x) * (1-sig(x))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 272
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "a = np.array([2,3,4])\n",
      "a.reshape((1,-1))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 155,
       "text": [
        "array([[2, 3, 4]])"
       ]
      }
     ],
     "prompt_number": 155
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "np.linalg.norm(a)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 208,
       "text": [
        "5.3851648071345037"
       ]
      }
     ],
     "prompt_number": 208
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}