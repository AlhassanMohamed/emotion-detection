{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# -*- coding: utf-8 -*-\n",
      "# <nbformat>3.0</nbformat>\n",
      "\n",
      "# <codecell>\n",
      "\n",
      "# import net\n",
      "from array import array\n",
      "import gzip\n",
      "import struct\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "# <codecell>\n",
      "\n",
      "# open images\n",
      "f = gzip.open('./train-images-idx3-ubyte.gz', 'rb') # test set\n",
      "_, n_images, rows, cols = struct.unpack('>IIII', f.read(16)) # first 16 bytes are metadata\n",
      "pixels = np.array(array('B', f.read())) / 255.0 # pixel data are unsigned bytes; normalize to [0,1] scale\n",
      "f.close()\n",
      "images = pixels.reshape((n_images, rows * cols)) # each image is a vector of 28*28 values in [0,1]\n",
      "\n",
      "# <codecell>\n",
      "\n",
      "f = gzip.open('./train-labels-idx1-ubyte.gz', 'rb')\n",
      "_, n_images = struct.unpack('>II', f.read(8)) # first 16 bytes are metadata\n",
      "digits = np.array(array('B', f.read())) \n",
      "f.close()\n",
      "\n",
      "# <codecell>\n",
      "\n",
      "labels = np.zeros((n_images, 10))\n",
      "for i in range(n_images):\n",
      "    labels[i][digits[i]] = 1"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# <codecell>\n",
      "temp_images = images[0:10]\n",
      "temp_labels = labels[0:10]\n",
      "train_data = zip(temp_images, temp_labels)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 57
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# <codecell>\n",
      "detector = Net([rows * cols, 400, 10])\n",
      "detector.SGD(train_data, 10, 10, 0.1)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "backprop little delta biases -0.377887467957\n",
        "backprop little delta biases -3.11282418864\n",
        "backprop little delta biases"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " -1.39370417043\n",
        "backprop little delta biases -1.09304922474\n",
        "backprop little delta biases"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.31887952594\n",
        "backprop little delta biases 1.60682328856\n",
        "backprop little delta biases"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " -0.419585846775\n",
        "backprop little delta biases -2.83470988268\n",
        "backprop little delta biases"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " -0.967031498568\n",
        "backprop little delta biases -2.37721824693\n",
        "backprop little delta biases"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " -0.967031498568\n",
        "backprop little delta biases -1.09304922474\n",
        "backprop little delta biases"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " -0.377887467957\n",
        "backprop little delta biases -1.39370417043\n",
        "backprop little delta biases"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 1.60682328856\n",
        "backprop little delta biases 0.31887952594\n",
        "backprop little delta biases"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " -2.83470988268\n",
        "backprop little delta biases -0.419585846775\n",
        "backprop little delta biases"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " -3.11282418864\n",
        "backprop little delta biases -2.37721824693\n",
        "backprop little delta biases"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " -2.83470988268\n",
        "backprop little delta biases -2.37721824693\n",
        "backprop little delta biases"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " -1.09304922474\n",
        "backprop little delta biases -0.377887467957\n",
        "backprop little delta biases"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " -0.419585846775\n",
        "backprop little delta biases -0.967031498568\n",
        "backprop little delta biases"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.31887952594\n",
        "backprop little delta biases -1.39370417043\n",
        "backprop little delta biases"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " -3.11282418864\n",
        "backprop little delta biases 1.60682328856\n",
        "backprop little delta biases"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " -0.967031498568\n",
        "backprop little delta biases -3.11282418864\n",
        "backprop little delta biases"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " -2.37721824693\n",
        "backprop little delta biases -0.419585846775\n",
        "backprop little delta biases"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.31887952594\n",
        "backprop little delta biases -2.83470988268\n",
        "backprop little delta biases"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " -1.09304922474\n",
        "backprop little delta biases"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " -0.377887467957\n",
        "backprop little delta biases 1.60682328856\n",
        "backprop little delta biases"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " -1.39370417043\n",
        "backprop little delta biases -3.11282418864\n",
        "backprop little delta biases"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.31887952594\n",
        "backprop little delta biases -0.419585846775\n",
        "backprop little delta biases"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " -0.377887467957\n",
        "backprop little delta biases -1.39370417043\n",
        "backprop little delta biases"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " -1.09304922474\n",
        "backprop little delta biases 1.60682328856\n",
        "backprop little delta biases"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " -2.83470988268\n",
        "backprop little delta biases -0.967031498568\n",
        "backprop little delta biases"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " -2.37721824693\n",
        "backprop little delta biases -3.11282418864\n",
        "backprop little delta biases"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " -2.83470988268\n",
        "backprop little delta biases -2.37721824693\n",
        "backprop little delta biases"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " -0.967031498568\n",
        "backprop little delta biases -0.377887467957\n",
        "backprop little delta biases"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.31887952594\n",
        "backprop little delta biases -0.419585846775\n",
        "backprop little delta biases"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 1.60682328856\n",
        "backprop little delta biases -1.39370417043\n",
        "backprop little delta biases"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " -1.09304922474\n",
        "backprop little delta biases -0.377887467957\n",
        "backprop little delta biases"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " -0.419585846775\n",
        "backprop little delta biases -1.39370417043\n",
        "backprop little delta biases"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 1.60682328856\n",
        "backprop little delta biases -3.11282418864\n",
        "backprop little delta biases"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " -2.83470988268\n",
        "backprop little delta biases -2.37721824693\n",
        "backprop little delta biases"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " -0.967031498568\n",
        "backprop little delta biases 0.31887952594\n",
        "backprop little delta biases"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " -1.09304922474\n",
        "backprop little delta biases 1.60682328856\n",
        "backprop little delta biases"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.31887952594\n",
        "backprop little delta biases -2.83470988268\n",
        "backprop little delta biases"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " -3.11282418864\n",
        "backprop little delta biases -0.967031498568\n",
        "backprop little delta biases"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " -2.37721824693\n",
        "backprop little delta biases -0.377887467957\n",
        "backprop little delta biases"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " -1.39370417043\n",
        "backprop little delta biases -0.419585846775\n",
        "backprop little delta biases"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " -1.09304922474\n",
        "backprop little delta biases 0.31887952594\n",
        "backprop little delta biases"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " -2.37721824693\n",
        "backprop little delta biases -1.09304922474\n",
        "backprop little delta biases"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " -1.39370417043\n",
        "backprop little delta biases -2.83470988268\n",
        "backprop little delta biases"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " -0.967031498568\n",
        "backprop little delta biases 1.60682328856\n",
        "backprop little delta biases"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " -0.419585846775\n",
        "backprop little delta biases -0.377887467957\n",
        "backprop little delta biases"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " -3.11282418864\n",
        "backprop little delta biases 0.31887952594\n",
        "backprop little delta biases"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " -1.09304922474\n",
        "backprop little delta biases -1.39370417043\n",
        "backprop little delta biases"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " -0.377887467957\n",
        "backprop little delta biases 1.60682328856\n",
        "backprop little delta biases"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " -2.83470988268\n",
        "backprop little delta biases -0.967031498568\n",
        "backprop little delta biases"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " -0.419585846775\n",
        "backprop little delta biases -2.37721824693\n",
        "backprop little delta biases"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " -3.11282418864\n"
       ]
      }
     ],
     "prompt_number": 65
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# test the neural network\n",
      "%pylab inline\n",
      "tests = images[1000:1064]\n",
      "plt.figure(figsize=(10, 10))\n",
      "for k in xrange(len(tests)):\n",
      "    plt.subplot2grid((8, 8), (k / 8, k % 8))\n",
      "    plt.axis('off')\n",
      "    plt.imshow(1.0 - tests[k].reshape((rows, cols)), cmap=plt.cm.gray)\n",
      "    # plt.title(detector.evaluate(tests[k]), fontsize=24)\n",
      "    print detector.evaluate(tests[k])\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 66
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import random\n",
      "import numpy as np\n",
      "\n",
      "class Net(object):\n",
      "    def __init__(self,sizes):\n",
      "        \"\"\"\n",
      "        Sizes is a list of the number of neurons in each layer of the neural \n",
      "        network. For example, if sizes = [3,2,3], that would indicate that the\n",
      "        neural network has three layers, the first with 3 neurons, the second \n",
      "        with 2 neurons, and the third with 3 neurons. \n",
      "\n",
      "        The function initializes the parameters and framework for the network.\n",
      "        \"\"\"\n",
      "\n",
      "        self.sizes = sizes\n",
      "        self.numlayers = len(sizes)\n",
      "        self.weights = [np.random.randn(self.sizes[1],self.sizes[0]), np.random.randn(self.sizes[2],self.sizes[1])]\n",
      "        self.biases = [np.random.randn(self.sizes[1]), np.random.randn(self.sizes[2])]\n",
      "\n",
      "\n",
      "    def SGD(self, train_data, epochs, mini_batch_size, eta):\n",
      "        \"\"\"\n",
      "        Trains the network using mini-batch stochastic gradient descent. \n",
      "        Parameters:\n",
      "            train_data: A vector of the data used to train the model. Each element\n",
      "                in the vector is of the form (x,y) where x is the training data and\n",
      "                y is the correct output. \n",
      "\n",
      "            epochs: The number of epochs in the training.\n",
      "\n",
      "            mini_batch_size: The number of mini-batches. If we have some set of \n",
      "                training data inputs, we partition the set of training data inputs\n",
      "                into sets of mini_batch_size. The gradient of the cost function\n",
      "                is calculated for the mini-batch, from which we calculate the \n",
      "                gradient for the entire training set and the new weights and biases\n",
      "                are calculated.\n",
      "\n",
      "            eta: The constant that determines how much the weights and biases\n",
      "                change in the direction of the calculated gradient. \n",
      "        \"\"\"\n",
      "        for i in range(epochs):\n",
      "            random.shuffle(train_data) # shuffle the data to randomly devide it into mini batches later\n",
      "            cur = 0\n",
      "            while cur < len(train_data):\n",
      "                mini_batch = train_data[cur : cur + mini_batch_size]\n",
      "                cur += mini_batch_size\n",
      "                self.update_weights(mini_batch, eta)\n",
      "\n",
      "#     def update_weights (self, mini_batch, eta):\n",
      "#         \"\"\"\n",
      "#         Updates the weights and biases by applying gradient descent to the \n",
      "#         mini-batch passed into the function. \n",
      "#         \"\"\"\n",
      "#         delta_weights = [np.zeros((self.sizes[1],self.sizes[0])), np.zeros((self.sizes[2],self.sizes[1]))]\n",
      "#         delta_biases = [np.zeros(self.sizes[1]), np.zeros(self.sizes[2])]\n",
      "        \n",
      "#         for x, y in mini_batch:\n",
      "#             delta_weights_temp, delta_biases_temp = self.backprop(x, y)\n",
      "#             print \"backprop little delta biases\", delta_biases_temp[1][0]\n",
      "#             for i in range(self.numlayers-1):\n",
      "#                 delta_weights[i] += delta_weights_temp[i]\n",
      "#                 delta_biases[i] += delta_biases_temp[i]\n",
      "        \n",
      "#         print \"summed delta biases\", delta_biases[1][0]\n",
      "        \n",
      "#         # adjust the weights\n",
      "#         for i in range(self.numlayers-1):\n",
      "#             self.weights[i] += - eta * delta_weights[i]\n",
      "#             if i == 1:\n",
      "#                 print \"before: biases\", self.biases[1][0]\n",
      "#                 print \"change: \", - eta* delta_biases[1]\n",
      "#             self.biases[i] += - eta * delta_biases[i]\n",
      "        \n",
      "#         print \"after: biases\", self.biases[1][0]\n",
      "\n",
      "    def update_mini_batch(self, mini_batch, eta):\n",
      "        \"\"\"Update the network's weights and biases by applying\n",
      "        gradient descent using backpropagation to a single mini batch.\n",
      "        The ``mini_batch`` is a list of tuples ``(x, y)``, and ``eta``\n",
      "        is the learning rate.\"\"\"\n",
      "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
      "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
      "        for x, y in mini_batch:\n",
      "            delta_nabla_w, delta_nabla_b = self.backprop(x, y)\n",
      "            print \"backprop little delta biases\", delta_biases_temp[1][0]\n",
      "            nabla_b = [nb+dnb for nb, dnb in zip(nabla_b, delta_nabla_b)]\n",
      "            nabla_w = [nw+dnw for nw, dnw in zip(nabla_w, delta_nabla_w)]\n",
      "        self.weights = [w-eta*nw for w, nw in zip(self.weights, nabla_w)]\n",
      "        self.biases = [b-eta*nb for b, nb in zip(self.biases, nabla_b)]\n",
      "        print \"after: biases\", self.biases[1][0]\n",
      "\n",
      "    def backprop(self,x,y):\n",
      "        \"\"\"\n",
      "        Uses backpropagation to calculate the gradient of the cost function\n",
      "\n",
      "        Backpropagation consists of the following five steps:\n",
      "            1) Set the input of the network to the training data. \n",
      "            \n",
      "            2) Use the feedforward function to compute the activations\n",
      "            at each layer of the network given the input.\n",
      "\n",
      "            3) Compute each error vector for the last layer before the output\n",
      "            layer by using the rate of change of the cost as a function of the \n",
      "            corresponding output and the derivative of the activation function\n",
      "            evaluated at the last layer neuron. \n",
      "\n",
      "            4) Using each error value in the last layer, calculate the error\n",
      "            values in each of the previous layers. \n",
      "\n",
      "            5) Calculate the gradient of the cost function for each weight and\n",
      "            for the biases using the error values, using the formula in the spec.\n",
      "            \n",
      "        Set activation to a list containing x  # x is the input activation\n",
      "        Set zlist to empty list  # zlist is the list of z activations\n",
      "        \"\"\"\n",
      "        sig_prime_vec = np.vectorize(sig_prime)\n",
      "        sig_vec = np.vectorize(sig)\n",
      "\n",
      "        activation = []\n",
      "        activation.append(x)\n",
      "        zlist = []\n",
      "        zlist.append(x)\n",
      "\n",
      "        numlayers = self.numlayers\n",
      "\n",
      "        for i in range(numlayers - 1):\n",
      "            a = np.dot(self.weights[i],zlist[i]) + self.biases[i]\n",
      "            z = sig_vec(a)\n",
      "            activation.append(a)\n",
      "            zlist.append(z)\n",
      "\n",
      "        output = z\n",
      "\n",
      "        gradcost = a - y  # Gradient of the cost function with respect to the activation outputs\n",
      "\n",
      "        delt_l = gradcost * sig_prime_vec(output) # Error delta for last layer\n",
      "        delts = []\n",
      "        delts.append(delt_l)\n",
      "\n",
      "        for i in range(2, numlayers + 1): # Calculate errors for previous layers\n",
      "\n",
      "            delt = (np.dot(np.transpose(self.weights[numlayers - i]),delts[i-2])*\n",
      "                    sig_prime_vec(zlist[numlayers - i]))\n",
      "            delts.append(delt)\n",
      "\n",
      "        delts = delts[::-1]\n",
      "        grad_biases = np.delete(delts,0) # Calculate the partial derivatives of the cost function wrt to biases\n",
      "\n",
      "        grad_weights = []\n",
      "        for i in range(1, numlayers): # Calculate the gradient with respect to the weights\n",
      "            gweight = np.dot(np.transpose(delts[i].reshape((1,-1))),activation[i-1].reshape((1,-1)))\n",
      "            grad_weights.append(gweight)\n",
      "\n",
      "        return(grad_weights,grad_biases)\n",
      "\n",
      "    def evaluate(self,test_data):\n",
      "        sig_prime_vec = np.vectorize(sig_prime)\n",
      "        sig_vec = np.vectorize(sig)\n",
      "\n",
      "        activation = []\n",
      "        activation.append(test_data)\n",
      "        zlist = []\n",
      "        zlist.append(test_data)\n",
      "\n",
      "        numlayers = self.numlayers\n",
      "\n",
      "        for i in range(numlayers - 1):\n",
      "            a = np.dot(self.weights[i],zlist[i]) + self.biases[i]\n",
      "            z = sig_vec(a)\n",
      "            activation.append(a)\n",
      "            zlist.append(z)\n",
      "\n",
      "        output = z\n",
      "\n",
      "        return output\n",
      "        # return max(enumerate(output),key=lambda x: x[1])[0] # return the index of the maximum element in output list\n",
      "\n",
      "\n",
      "def sig(x):\n",
      "    return 1.0/(1.0 + np.exp(-x))\n",
      "\n",
      "def sig_prime(x):\n",
      "    return sig(x) * (1-sig(x))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 64
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 56
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}