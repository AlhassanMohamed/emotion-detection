{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# -*- coding: utf-8 -*-\n",
      "# <nbformat>3.0</nbformat>\n",
      "\n",
      "# <codecell>\n",
      "\n",
      "# import net\n",
      "from array import array\n",
      "import gzip\n",
      "import struct\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "# <codecell>\n",
      "\n",
      "# open images\n",
      "f = gzip.open('./train-images-idx3-ubyte.gz', 'rb') # test set\n",
      "_, n_images, rows, cols = struct.unpack('>IIII', f.read(16)) # first 16 bytes are metadata\n",
      "pixels = np.array(array('B', f.read())) / 255.0 # pixel data are unsigned bytes; normalize to [0,1] scale\n",
      "f.close()\n",
      "images = pixels.reshape((n_images, rows * cols)) # each image is a vector of 28*28 values in [0,1]\n",
      "\n",
      "# <codecell>\n",
      "\n",
      "f = gzip.open('./train-labels-idx1-ubyte.gz', 'rb')\n",
      "_, n_images = struct.unpack('>II', f.read(8)) # first 16 bytes are metadata\n",
      "digits = np.array(array('B', f.read())) \n",
      "f.close()\n",
      "\n",
      "# <codecell>\n",
      "\n",
      "labels = np.zeros((n_images, 10))\n",
      "for i in range(n_images):\n",
      "    labels[i][digits[i]] = 1"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# <codecell>\n",
      "temp_images = images[0:10]\n",
      "temp_labels = labels[0:10]\n",
      "train_data = zip(temp_images, temp_labels)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 57
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# <codecell>\n",
      "detector = Net([rows * cols, 400, 10])\n",
      "detector.SGD(train_data, 10, 10, 0.1)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "backprop little delta biases -6.62956335938\n",
        "backprop little delta biases -6.50494687506\n",
        "backprop little delta biases"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " -3.43762583891\n",
        "backprop little delta biases -5.18000455479\n",
        "backprop little delta biases"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " -8.17918798051\n",
        "backprop little delta biases -4.74971476693\n",
        "backprop little delta biases"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " -7.6498498306\n",
        "backprop little delta biases -4.12309137215\n",
        "backprop little delta biases"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " -3.97361534213\n",
        "backprop little delta biases -7.07037555373\n",
        "summed delta biases -57.4979754742\n",
        "before: biases -0.66671578541\n",
        "change:  [ 5.74979755 -1.31647774 -3.12522125  1.14618273  0.48734987 -1.39192912\n",
        "  1.78509368 -5.27793353 -2.69064467 -3.73217735]\n",
        "after: biases 5.08308176201\n",
        "backprop little delta biases"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 34.6425240315\n",
        "backprop little delta biases 82.4663168289\n",
        "backprop little delta biases"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 52.8275468646\n",
        "backprop little delta biases 62.9591354783\n",
        "backprop little delta biases"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 46.9360358322\n",
        "backprop little delta biases 93.2592951516\n",
        "backprop little delta biases"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 18.2890021972\n",
        "backprop little delta biases 39.1010455318\n",
        "backprop little delta biases"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 18.2266013279\n",
        "backprop little delta biases 63.2053830056\n",
        "summed delta biases 511.91288625\n",
        "before: biases 5.08308176201\n",
        "change:  [-51.19128862   6.33629078  42.95226851   1.21712904  -5.26055306\n",
        "  10.93142553 -21.51807673  78.32923138  37.94436646  48.11797238]\n",
        "after: biases -46.108206863\n",
        "backprop little delta biases"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 46385.5455282\n",
        "backprop little delta biases 45939.332273\n",
        "backprop little delta biases"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 46385.5455282\n",
        "backprop little delta biases 46385.3489163\n",
        "backprop little delta biases"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 46385.5455282\n",
        "backprop little delta biases 46385.5455282\n",
        "backprop little delta biases"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 46385.5455282\n",
        "backprop little delta biases 46385.5455282\n",
        "backprop little delta biases"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 45942.0047005\n",
        "backprop little delta biases 46385.5455282\n",
        "summed delta biases 462965.504587\n",
        "before: biases -46.108206863\n",
        "change:  [-46296.55045873   7899.45521825  49611.71451357   1364.52392404\n",
        "  -4433.90547659  12220.07339695 -18659.15171804  88596.887531\n",
        "  43694.94313491  54610.0781821 ]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "after: biases -46342.6586656\n",
        "backprop little delta biases 4881582773.03\n",
        "backprop little delta biases"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 4881582773.03\n",
        "backprop little delta biases 4881582773.03\n",
        "backprop little delta biases"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 4881582773.03\n",
        "backprop little delta biases 4881582772.83\n",
        "backprop little delta biases"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 4881582773.03\n",
        "backprop little delta biases 4881582773.03\n",
        "backprop little delta biases"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 4881582773.03\n",
        "backprop little delta biases 4881582773.03\n",
        "backprop little delta biases"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 4881582773.03\n",
        "summed delta biases 48815827730.1\n",
        "before: biases -46342.6586656\n",
        "change:  [ -4.88158277e+09   1.05916392e+09   6.65158480e+09   1.82895283e+08\n",
        "  -4.67505425e+08   1.63840671e+09  -1.96739854e+09   1.18783226e+10\n",
        "   5.85830693e+09   7.32170917e+09]\n",
        "after: biases -4881629115.67\n",
        "backprop little delta biases 5.0098651652e+20\n",
        "backprop little delta biases"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 5.0098651652e+20\n",
        "backprop little delta biases 5.0098651652e+20\n",
        "backprop little delta biases"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 5.0098651652e+20\n",
        "backprop little delta biases 5.0098651652e+20\n",
        "backprop little delta biases"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 5.0098651652e+20\n",
        "backprop little delta biases 5.0098651652e+20\n",
        "backprop little delta biases"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 5.0098651652e+20\n",
        "backprop little delta biases 5.0098651652e+20\n",
        "backprop little delta biases"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 5.0098651652e+20\n",
        "summed delta biases 5.0098651652e+21\n",
        "before: biases -4881629115.67\n",
        "change:  [ -5.00986517e+20   1.38216119e+20   8.68001843e+20   2.38670102e+19\n",
        "  -4.79790931e+19   2.13804693e+20  -2.01909952e+20   1.55006757e+21\n",
        "   7.64482656e+20   9.55450054e+20]\n",
        "after: biases -5.00986516524e+20\n",
        "backprop little delta biases"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 8.61148513216e+41\n",
        "backprop little delta biases 8.61148513216e+41\n",
        "backprop little delta biases"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 8.61148513216e+41\n",
        "backprop little delta biases 8.61148513216e+41\n",
        "backprop little delta biases"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 8.61148513216e+41\n",
        "backprop little delta biases 8.61148513216e+41\n",
        "backprop little delta biases"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 8.61148513216e+41\n",
        "backprop little delta biases 8.61148513216e+41\n",
        "backprop little delta biases"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 8.61148513216e+41\n",
        "backprop little delta biases 8.61148513216e+41\n",
        "summed delta biases 8.61148513216e+42\n",
        "before: biases -5.00986516524e+20\n",
        "change:  [ -8.61148513e+41   3.02093128e+41   1.89715494e+42   5.21651155e+40\n",
        "  -8.24715303e+40   4.67303880e+41  -3.47064140e+41   3.38791717e+42\n",
        "   1.67089743e+42   2.08828680e+42]\n",
        "after: biases -8.61148513216e+41\n",
        "backprop little delta biases"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 2.41148816869e+85\n",
        "backprop little delta biases 2.41148816869e+85\n",
        "backprop little delta biases"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 2.41148816869e+85\n",
        "backprop little delta biases 2.41148816869e+85\n",
        "backprop little delta biases"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 2.41148816869e+85\n",
        "backprop little delta biases 2.41148816869e+85\n",
        "backprop little delta biases"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 2.41148816869e+85\n",
        "backprop little delta biases 2.41148816869e+85\n",
        "backprop little delta biases"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 2.41148816869e+85\n",
        "backprop little delta biases 2.41148816869e+85\n",
        "summed delta biases 2.41148816869e+86\n",
        "before: biases -8.61148513216e+41\n",
        "change:  [ -2.41148817e+85   1.07566754e+85   6.75522806e+85   1.85745109e+84\n",
        "  -2.30946366e+84   1.66393594e+85  -9.71889349e+84   1.20634075e+86\n",
        "   5.94958954e+85   7.43579413e+85]\n",
        "after: biases -2.41148816869e+85\n",
        "backprop little delta biases"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 3.13558497006e+171\n",
        "backprop little delta biases 3.13558497006e+171\n",
        "backprop little delta biases"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 3.13558497006e+171\n",
        "backprop little delta biases 3.13558497006e+171\n",
        "backprop little delta biases"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 3.13558497006e+171\n",
        "backprop little delta biases 3.13558497006e+171\n",
        "backprop little delta biases"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 3.13558497006e+171\n",
        "backprop little delta biases"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 3.13558497006e+171\n",
        "backprop little delta biases 3.13558497006e+171\n",
        "backprop little delta biases"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 3.13558497006e+171\n",
        "summed delta biases 3.13558497006e+172\n",
        "before: biases -2.41148816869e+85\n",
        "change:  [ -3.13558497e+171   1.77844990e+171   1.11687248e+172   3.07100810e+170\n",
        "  -3.00292559e+170   2.75106071e+171  -1.26371826e+171   1.99449785e+172\n",
        "   9.83672610e+171   1.22939355e+172]\n",
        "after: biases -3.13558497006e+171\n",
        "backprop little delta biases nan\n",
        "backprop little delta biases"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " nan\n",
        "backprop little delta biases nan\n",
        "backprop little delta biases"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " nan\n",
        "backprop little delta biases nan\n",
        "backprop little delta biases"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " nan\n",
        "backprop little delta biases nan\n",
        "backprop little delta biases nan\n",
        "backprop little delta biases"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " nan\n",
        "backprop little delta biases nan\n",
        "summed delta biases nan\n",
        "before: biases -3.13558497006e+171\n",
        "change:  [ nan  nan  nan  nan  nan  nan  nan  nan  nan  nan]\n",
        "after: biases nan\n",
        "backprop little delta biases nan\n",
        "backprop little delta biases"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " nan\n",
        "backprop little delta biases nan\n",
        "backprop little delta biases nan\n",
        "backprop little delta biases"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " nan\n",
        "backprop little delta biases nan\n",
        "backprop little delta biases nan\n",
        "backprop little delta biases"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " nan\n",
        "backprop little delta biases nan\n",
        "backprop little delta biases nan\n",
        "summed delta biases nan\n",
        "before: biases nan\n",
        "change:  [ nan  nan  nan  nan  nan  nan  nan  nan  nan  nan]\n",
        "after: biases nan\n"
       ]
      }
     ],
     "prompt_number": 68
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# test the neural network\n",
      "%pylab inline\n",
      "tests = images[1000:1064]\n",
      "plt.figure(figsize=(10, 10))\n",
      "for k in xrange(len(tests)):\n",
      "    plt.subplot2grid((8, 8), (k / 8, k % 8))\n",
      "    plt.axis('off')\n",
      "    plt.imshow(1.0 - tests[k].reshape((rows, cols)), cmap=plt.cm.gray)\n",
      "    # plt.title(detector.evaluate(tests[k]), fontsize=24)\n",
      "    print detector.evaluate(tests[k])\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 70
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import random\n",
      "import numpy as np\n",
      "\n",
      "class Net(object):\n",
      "    def __init__(self,sizes):\n",
      "        \"\"\"\n",
      "        Sizes is a list of the number of neurons in each layer of the neural \n",
      "        network. For example, if sizes = [3,2,3], that would indicate that the\n",
      "        neural network has three layers, the first with 3 neurons, the second \n",
      "        with 2 neurons, and the third with 3 neurons. \n",
      "\n",
      "        The function initializes the parameters and framework for the network.\n",
      "        \"\"\"\n",
      "\n",
      "        self.sizes = sizes\n",
      "        self.numlayers = len(sizes)\n",
      "        self.weights = [np.random.randn(self.sizes[1],self.sizes[0]), np.random.randn(self.sizes[2],self.sizes[1])]\n",
      "        self.biases = [np.random.randn(self.sizes[1]), np.random.randn(self.sizes[2])]\n",
      "\n",
      "\n",
      "    def SGD(self, train_data, epochs, mini_batch_size, eta):\n",
      "        \"\"\"\n",
      "        Trains the network using mini-batch stochastic gradient descent. \n",
      "        Parameters:\n",
      "            train_data: A vector of the data used to train the model. Each element\n",
      "                in the vector is of the form (x,y) where x is the training data and\n",
      "                y is the correct output. \n",
      "\n",
      "            epochs: The number of epochs in the training.\n",
      "\n",
      "            mini_batch_size: The number of mini-batches. If we have some set of \n",
      "                training data inputs, we partition the set of training data inputs\n",
      "                into sets of mini_batch_size. The gradient of the cost function\n",
      "                is calculated for the mini-batch, from which we calculate the \n",
      "                gradient for the entire training set and the new weights and biases\n",
      "                are calculated.\n",
      "\n",
      "            eta: The constant that determines how much the weights and biases\n",
      "                change in the direction of the calculated gradient. \n",
      "        \"\"\"\n",
      "        for i in range(epochs):\n",
      "            random.shuffle(train_data) # shuffle the data to randomly devide it into mini batches later\n",
      "            cur = 0\n",
      "            while cur < len(train_data):\n",
      "                mini_batch = train_data[cur : cur + mini_batch_size]\n",
      "                cur += mini_batch_size\n",
      "                self.update_weights(mini_batch, eta)\n",
      "\n",
      "    def update_weights (self, mini_batch, eta):\n",
      "        \"\"\"\n",
      "        Updates the weights and biases by applying gradient descent to the \n",
      "        mini-batch passed into the function. \n",
      "        \"\"\"\n",
      "        delta_weights = [np.zeros((self.sizes[1],self.sizes[0])), np.zeros((self.sizes[2],self.sizes[1]))]\n",
      "        delta_biases = [np.zeros(self.sizes[1]), np.zeros(self.sizes[2])]\n",
      "        \n",
      "        for x, y in mini_batch:\n",
      "            delta_weights_temp, delta_biases_temp = self.backprop(x, y)\n",
      "            print \"backprop little delta biases\", delta_biases_temp[1][0]\n",
      "            for i in range(self.numlayers-1):\n",
      "                delta_weights[i] += delta_weights_temp[i]\n",
      "                delta_biases[i] += delta_biases_temp[i]\n",
      "        \n",
      "        print \"summed delta biases\", delta_biases[1][0]\n",
      "        \n",
      "        # adjust the weights\n",
      "        for i in range(self.numlayers-1):\n",
      "            self.weights[i] += - eta * delta_weights[i]\n",
      "            if i == 1:\n",
      "                print \"before: biases\", self.biases[1][0]\n",
      "                print \"change: \", - eta* delta_biases[1]\n",
      "            self.biases[i] += - eta * delta_biases[i]\n",
      "        \n",
      "        print \"after: biases\", self.biases[1][0]\n",
      "\n",
      "    def backprop(self,x,y):\n",
      "        \"\"\"\n",
      "        Uses backpropagation to calculate the gradient of the cost function\n",
      "\n",
      "        Backpropagation consists of the following five steps:\n",
      "            1) Set the input of the network to the training data. \n",
      "            \n",
      "            2) Use the feedforward function to compute the activations\n",
      "            at each layer of the network given the input.\n",
      "\n",
      "            3) Compute each error vector for the last layer before the output\n",
      "            layer by using the rate of change of the cost as a function of the \n",
      "            corresponding output and the derivative of the activation function\n",
      "            evaluated at the last layer neuron. \n",
      "\n",
      "            4) Using each error value in the last layer, calculate the error\n",
      "            values in each of the previous layers. \n",
      "\n",
      "            5) Calculate the gradient of the cost function for each weight and\n",
      "            for the biases using the error values, using the formula in the spec.\n",
      "            \n",
      "        Set activation to a list containing x  # x is the input activation\n",
      "        Set zlist to empty list  # zlist is the list of z activations\n",
      "        \"\"\"\n",
      "        sig_prime_vec = np.vectorize(sig_prime)\n",
      "        sig_vec = np.vectorize(sig)\n",
      "\n",
      "        activation = []\n",
      "        activation.append(x)\n",
      "        zlist = []\n",
      "        zlist.append(x)\n",
      "\n",
      "        numlayers = self.numlayers\n",
      "\n",
      "        for i in range(numlayers - 1):\n",
      "            a = np.dot(self.weights[i],zlist[i]) + self.biases[i]\n",
      "            z = sig_vec(a)\n",
      "            activation.append(a)\n",
      "            zlist.append(z)\n",
      "\n",
      "        output = z\n",
      "\n",
      "        gradcost = a - y  # Gradient of the cost function with respect to the activation outputs\n",
      "\n",
      "        delt_l = gradcost * sig_prime_vec(output) # Error delta for last layer\n",
      "        delts = []\n",
      "        delts.append(delt_l)\n",
      "\n",
      "        for i in range(2, numlayers + 1): # Calculate errors for previous layers\n",
      "\n",
      "            delt = (np.dot(np.transpose(self.weights[numlayers - i]),delts[i-2])*\n",
      "                    sig_prime_vec(zlist[numlayers - i]))\n",
      "            delts.append(delt)\n",
      "\n",
      "        delts = delts[::-1]\n",
      "        grad_biases = np.delete(delts,0) # Calculate the partial derivatives of the cost function wrt to biases\n",
      "\n",
      "        grad_weights = []\n",
      "        for i in range(1, numlayers): # Calculate the gradient with respect to the weights\n",
      "            gweight = np.dot(np.transpose(delts[i].reshape((1,-1))),activation[i-1].reshape((1,-1)))\n",
      "            grad_weights.append(gweight)\n",
      "\n",
      "        return(grad_weights,grad_biases)\n",
      "\n",
      "    def evaluate(self,test_data):\n",
      "        sig_prime_vec = np.vectorize(sig_prime)\n",
      "        sig_vec = np.vectorize(sig)\n",
      "\n",
      "        activation = []\n",
      "        activation.append(test_data)\n",
      "        zlist = []\n",
      "        zlist.append(test_data)\n",
      "\n",
      "        numlayers = self.numlayers\n",
      "\n",
      "        for i in range(numlayers - 1):\n",
      "            a = np.dot(self.weights[i],zlist[i]) + self.biases[i]\n",
      "            z = sig_vec(a)\n",
      "            activation.append(a)\n",
      "            zlist.append(z)\n",
      "\n",
      "        output = z\n",
      "\n",
      "        return output\n",
      "        # return max(enumerate(output),key=lambda x: x[1])[0] # return the index of the maximum element in output list\n",
      "\n",
      "\n",
      "def sig(x):\n",
      "    return 1.0/(1.0 + np.exp(-x))\n",
      "\n",
      "def sig_prime(x):\n",
      "    return sig(x) * (1-sig(x))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 67
    }
   ],
   "metadata": {}
  }
 ]
}